import mcdc.mcdc_get as mcdc_get
import mcdc.physics.interface as physics

from mpi4py import MPI
from numba import njit, objmode
from numba.misc.special import literally

import mcdc.config as config
import mcdc.adapt as adapt
import mcdc.geometry.interface as geometry
import mcdc.kernel as kernel
import mcdc.print_ as print_module
import mcdc.type_ as type_

from mcdc.constant import *
from mcdc.print_ import (
    print_header_batch,
    print_iqmc_eigenvalue_progress,
    print_iqmc_eigenvalue_exit_code,
    print_msg,
    print_progress,
    print_progress_eigenvalue,
    print_progress_iqmc,
)

caching = config.caching


# =============================================================================
# Functions for GPU Interop
# =============================================================================

# The symbols declared below will be overwritten to reference external code that
# manages GPU execution (if GPU execution is supported and selected)
alloc_state, free_state = [None] * 2

src_alloc_program, src_free_program = [None] * 2
(
    src_load_global,
    src_load_constant,
    src_store_global,
    src_store_data,
    src_store_pointer_data,
) = [None] * 5
src_init_program, src_exec_program, src_complete, src_clear_flags = [None] * 4

pre_alloc_program, pre_free_program = [None] * 2
pre_load_global, pre_load_data, pre_store_global, pre_store_data = [None] * 4
pre_init_program, pre_exec_program, pre_complete, pre_clear_flags = [None] * 4


# If GPU execution is supported and selected, the functions shown below will
# be redefined to overwrite the above symbols and perform initialization/
# finalization of GPU state
@njit
def setup_gpu(mcdc, data_tally):
    pass


@njit
def teardown_gpu(mcdc):
    pass


# =========================================================================
# Fixed-source loop
# =========================================================================


@njit
def loop_fixed_source(data_tally, mcdc_arr, data):
    # Ensure `mcdc` exist for the lifetime of the program
    # by intentionally leaking their memory
    # adapt.leak(mcdc_arr)
    mcdc = mcdc_arr[0]

    # Get some settings
    settings = mcdc["settings"]
    N_batch = settings["N_batch"]
    N_particle = settings["N_particle"]
    N_census = settings["N_census"]
    use_census_based_tally = settings["use_census_based_tally"]

    # Loop over batches
    for i_batch in range(N_batch):
        if not mcdc["technique"]["domain_decomposition"]:
            kernel.distribute_work(N=N_particle, mcdc=mcdc)
        else:
            kernel.distribute_work_dd(N_particle, mcdc=mcdc)
        mcdc["idx_batch"] = i_batch
        seed_batch = kernel.split_seed(i_batch, settings["rng_seed"])

        # Print multi-batch header
        if N_batch > 1:
            with objmode():
                print_header_batch(i_batch, N_batch)
            # TODO
            # if mcdc["technique"]["uq"]:
            #    seed_uq = kernel.split_seed(seed_batch, SEED_SPLIT_UQ)
            #    kernel.uq_reset(mcdc, seed_uq)

        # Loop over time censuses
        for i_census in range(N_census):
            mcdc["idx_census"] = i_census
            seed_census = kernel.split_seed(seed_batch, SEED_SPLIT_CENSUS)

            # Set census-based tally time grids
            if use_census_based_tally:
                N_bin = settings["census_tally_frequency"]
                if i_census == 0:
                    t_start = 0.0
                else:
                    t_start = mcdc_get.settings.census_time(
                        i_census - 1, settings, data
                    )
                t_end = mcdc_get.settings.census_time(i_census, settings, data)
                dt = (t_end - t_start) / N_bin
                for tally in mcdc["mesh_tallies"]:
                    tally["filter"]["t"][0] = t_start
                    for i in range(N_bin):
                        tally["filter"]["t"][i + 1] = tally["filter"]["t"][i] + dt

            # Check and accordingly promote future particles to censused particle
            if kernel.get_bank_size(mcdc["bank_future"]) > 0:
                kernel.check_future_bank(mcdc, data)
            if (
                i_census > 0
                and kernel.get_bank_size(mcdc["bank_source"]) == 0
                and kernel.get_bank_size(mcdc["bank_census"]) == 0
                and kernel.get_bank_size(mcdc["bank_future"]) == 0
            ):
                # No more particle to work on
                break
            # Loop over source particles
            seed_source = kernel.split_seed(seed_census, SEED_SPLIT_SOURCE)
            loop_source(seed_source, data_tally, mcdc, data)

            # Manage particle banks: population control and work rebalance
            seed_bank = kernel.split_seed(seed_census, SEED_SPLIT_BANK)
            kernel.manage_particle_banks(seed_bank, mcdc)

            # Time census-based tally closeout
            if use_census_based_tally:
                kernel.tally_reduce(data_tally, mcdc)
                if mcdc["mpi_master"]:
                    kernel.census_based_tally_output(data_tally, mcdc)
                    if mcdc["technique"]["weight_window"] and i_census < N_census - 2:
                        kernel.update_weight_windows(data_tally, mcdc)
                # TODO: UQ tally

        # Multi-batch closeout
        if N_batch > 1:
            # Reset banks
            kernel.set_bank_size(mcdc["bank_active"], 0)
            kernel.set_bank_size(mcdc["bank_census"], 0)
            kernel.set_bank_size(mcdc["bank_source"], 0)
            kernel.set_bank_size(mcdc["bank_future"], 0)

            # DD closeout
            if mcdc["technique"]["domain_decomposition"]:
                mcdc["dd_N_local_source"] = 0
                mcdc["domain_decomp"]["work_done"] = False

            if not use_census_based_tally:
                # Tally history closeout
                kernel.tally_reduce(data_tally, mcdc)
                kernel.tally_accumulate(data_tally, mcdc)

                # Uq closeout
                if mcdc["technique"]["uq"]:
                    kernel.uq_tally_closeout_batch(data_tally, mcdc)

    # Tally closeout
    if not use_census_based_tally:
        if mcdc["technique"]["uq"]:
            kernel.uq_tally_closeout(data_tally, mcdc)
        kernel.tally_closeout(data_tally, mcdc)


# =========================================================================
# Eigenvalue loop
# =========================================================================


@njit
def loop_eigenvalue(data_tally, mcdc_arr, data):
    # Ensure `mcdc` exist for the lifetime of the program
    # by intentionally leaking their memory
    # adapt.leak(mcdc_arr)
    mcdc = mcdc_arr[0]

    settings = mcdc["settings"]

    # Loop over power iteration cycles
    N_active = settings["N_active"]
    N_inactive = settings["N_inactive"]
    N_cycle = N_active + N_inactive
    for idx_cycle in range(N_cycle):
        seed_cycle = kernel.split_seed(idx_cycle, settings["rng_seed"])

        # Loop over source particles
        seed_source = kernel.split_seed(seed_cycle, SEED_SPLIT_SOURCE)
        loop_source(seed_source, data_tally, mcdc, data)

        # Tally "history" closeout
        kernel.eigenvalue_tally_closeout_history(mcdc)
        if mcdc["cycle_active"]:
            kernel.tally_reduce(data_tally, mcdc)
            kernel.tally_accumulate(data_tally, mcdc)

        # DD closeout
        if mcdc["technique"]["domain_decomposition"]:
            mcdc["dd_N_local_source"] = 0
            mcdc["domain_decomp"]["work_done"] = False

        # Print progress
        with objmode():
            print_progress_eigenvalue(mcdc)

        # Manage particle banks
        seed_bank = kernel.split_seed(seed_cycle, SEED_SPLIT_BANK)
        kernel.manage_particle_banks(seed_bank, mcdc)

        # Entering active cycle?
        mcdc["idx_cycle"] += 1
        if mcdc["idx_cycle"] >= N_inactive:
            mcdc["cycle_active"] = True

    # Tally closeout
    kernel.tally_closeout(data_tally, mcdc)
    kernel.eigenvalue_tally_closeout(mcdc)


# =============================================================================
# Source loop
# =============================================================================


@njit
def generate_source_particle(work_start, idx_work, seed, prog, data):
    mcdc = adapt.mcdc_global(prog)
    settings = mcdc["settings"]

    seed_work = kernel.split_seed(work_start + idx_work, seed)

    # =====================================================================
    # Get a source particle and put into active bank
    # =====================================================================

    P_arr = adapt.local_array(1, type_.particle_record)
    P = P_arr[0]

    # Get from fixed-source?
    if kernel.get_bank_size(mcdc["bank_source"]) == 0:
        # Sample source
        kernel.source_particle(P_arr, seed_work, mcdc)

    # Get from source bank
    else:
        P_arr = mcdc["bank_source"]["particles"][idx_work : (idx_work + 1)]
        P = P_arr[0]

    # Skip if beyond time boundary
    if P["t"] > settings["time_boundary"]:
        return

    # If domain is decomposed, check if particle is in the domain
    if mcdc["technique"]["domain_decomposition"]:
        if not kernel.particle_in_domain(P_arr, mcdc):
            return

        # Also check if it belongs to the current rank
        mcdc["dd_N_local_source"] += 1
        if mcdc["technique"]["dd_work_ratio"][mcdc["dd_idx"]] > 1:
            if (
                mcdc["dd_N_local_source"]
                % mcdc["technique"]["dd_work_ratio"][mcdc["dd_idx"]]
                != mcdc["dd_local_rank"]
            ):
                return

    # Check if it is beyond current or next census times
    hit_census = False
    hit_next_census = False
    idx_census = mcdc["idx_census"]
    if idx_census < settings["N_census"] - 1:
        if P["t"] > mcdc_get.settings.census_time(idx_census + 1, settings, data):
            hit_census = True
            hit_next_census = True
        elif P["t"] > mcdc_get.settings.census_time(idx_census, settings, data):
            hit_census = True

    # Put into the right bank
    if not hit_census:
        adapt.add_active(P_arr, prog)
    elif not hit_next_census:
        # Particle will participate after the current census
        adapt.add_census(P_arr, prog)
    else:
        # Particle will participate in the future
        adapt.add_future(P_arr, prog)

    """
        if mcdc["technique"]["domain_decomposition"]:
            if mcdc["technique"]["dd_work_ratio"][mcdc["dd_idx"]] > 0:
                P["w"] /= mcdc["technique"]["dd_work_ratio"][mcdc["dd_idx"]]
            if kernel.particle_in_domain(P_arr, mcdc):
                adapt.add_census(P_arr, prog)
        else:
            adapt.add_census(P_arr, prog)
    else:
        P_new_arr = adapt.local_array(1, type_.particle)
        P_new = P_new_arr[0]
        # Add the source particle into the active bank
        if mcdc["technique"]["domain_decomposition"]:
            if mcdc["technique"]["dd_work_ratio"][mcdc["dd_idx"]] > 0:
                P["w"] /= mcdc["technique"]["dd_work_ratio"][mcdc["dd_idx"]]
            if kernel.particle_in_domain(P_arr, mcdc):
                kernel.recordlike_to_particle(P_new_arr, P_arr)
                adapt.add_active(P_new_arr, prog)
        else:
            kernel.recordlike_to_particle(P_new_arr, P_arr)
            adapt.add_active(P_new_arr, prog)
    """


@njit
def prep_particle(P_arr, prog):
    P = P_arr[0]
    mcdc = adapt.mcdc_global(prog)

    # Apply weight window
    if mcdc["technique"]["weight_window"]:
        kernel.weight_window(P_arr, prog)


@njit
def exhaust_active_bank(data_tally, prog, data):
    mcdc = adapt.mcdc_global(prog)
    P_arr = adapt.local_array(1, type_.particle)
    P = P_arr[0]

    # Loop until active bank is exhausted
    while kernel.get_bank_size(mcdc["bank_active"]) > 0:
        # Get particle from active bank
        kernel.get_particle(P_arr, mcdc["bank_active"], mcdc)

        prep_particle(P_arr, prog)

        # Particle loop
        loop_particle(P_arr, data_tally, mcdc, data)


@njit
def source_closeout(prog, idx_work, N_prog, data_tally):
    mcdc = adapt.mcdc_global(prog)

    # Tally history closeout for one-batch fixed-source simulation
    if not mcdc["settings"]["eigenvalue_mode"] and mcdc["settings"]["N_batch"] == 1:
        if not mcdc["settings"]["use_census_based_tally"]:
            kernel.tally_accumulate(data_tally, mcdc)

    # Tally history closeout for multi-batch uq simulation
    if mcdc["technique"]["uq"]:
        kernel.uq_tally_closeout_history(data_tally, mcdc)

    # Progress printout
    percent = (idx_work + 1.0) / mcdc["mpi_work_size"]
    if mcdc["settings"]["use_progress_bar"] and int(percent * 100.0) > N_prog:
        N_prog += 1
        with objmode():
            print_progress(percent, mcdc)


@njit
def source_dd_resolution(data, prog):
    mcdc = adapt.mcdc_global(prog)

    kernel.dd_particle_send(mcdc)
    terminated = False
    max_work = 1
    kernel.dd_recv(mcdc)
    if mcdc["domain_decomp"]["work_done"]:
        terminated = True

    P_arr = adapt.local_array(1, type_.particle)
    P = P_arr[0]

    while not terminated:
        if kernel.get_bank_size(mcdc["bank_active"]) > 0:
            # Loop until active bank is exhausted
            while kernel.get_bank_size(mcdc["bank_active"]) > 0:

                kernel.get_particle(P_arr, mcdc["bank_active"], mcdc)
                if not kernel.particle_in_domain(P_arr, mcdc) and P["alive"] == True:
                    print(f"recieved particle not in domain")

                # Apply weight window
                if mcdc["technique"]["weight_window"]:
                    kernel.weight_window(P_arr, mcdc)

                # Particle loop
                loop_particle(P_arr, data, mcdc)

                # Tally history closeout for one-batch fixed-source simulation
                if (
                    not mcdc["setting"]["mode_eigenvalue"]
                    and mcdc["setting"]["N_batch"] == 1
                ):
                    kernel.tally_accumulate(data, mcdc)

        # Send all domain particle banks
        kernel.dd_particle_send(mcdc)

        kernel.dd_recv(mcdc)

        # Progress printout
        """
        percent = 1 - work_remaining / max_work
        if mcdc["setting"]["progress_bar"] and int(percent * 100.0) > N_prog:
            N_prog += 1
            with objmode():
                print_progress(percent, mcdc)
        """
        if kernel.dd_check_halt(mcdc):
            kernel.dd_check_out(mcdc)
            terminated = True


@njit
def loop_source(seed, data_tally, mcdc, data):
    # Progress bar indicator
    N_prog = 0

    if mcdc["technique"]["domain_decomposition"]:
        kernel.dd_check_in(mcdc)

    # Loop over particle sources
    work_start = mcdc["mpi_work_start"]
    work_size = mcdc["mpi_work_size"]
    work_end = work_start + work_size

    for idx_work in range(work_size):
        generate_source_particle(work_start, idx_work, seed, mcdc, data)

        # Run the source particle and its secondaries
        exhaust_active_bank(data_tally, mcdc, data)

        source_closeout(mcdc, idx_work, N_prog, data_tally)

    if mcdc["technique"]["domain_decomposition"]:
        source_dd_resolution(data_tally, mcdc)


def gpu_sources_spec():
    def make_work(prog: nb.uintp) -> nb.boolean:
        mcdc = adapt.mcdc_global(prog)

        idx_work = adapt.global_add(mcdc["mpi_work_iter"], 0, 1)

        if idx_work >= mcdc["mpi_work_size"]:
            return False

        generate_source_particle(
            mcdc["mpi_work_start"], nb.uint64(idx_work), mcdc["source_seed"], prog
        )
        return True

    def initialize(prog: nb.uintp):
        pass

    def finalize(prog: nb.uintp):
        pass

    base_fns = (initialize, finalize, make_work)

    shape = eval(f"{adapt.tally_shape_literal}")

    # Just do exec/eval
    def step(prog: nb.uintp, P_input: adapt.particle_gpu):
        mcdc = adapt.mcdc_global(prog)
        data_ptr = adapt.mcdc_data(prog)
        data = adapt.harm.array_from_ptr(data_ptr, shape, nb.float64)
        P_arr = adapt.local_array(1, type_.particle)
        P_arr[0] = P_input
        P = P_arr[0]
        if P["fresh"]:
            prep_particle(P_arr, prog)
        P["fresh"] = False
        step_particle(P_arr, data, prog)
        if P["alive"]:
            adapt.step_async(prog, P)

    async_fns = [step]
    return adapt.harm.RuntimeSpec("mcdc_source", adapt.state_spec, base_fns, async_fns)


BLOCK_COUNT = config.args.gpu_block_count

ASYNC_EXECUTION = config.args.gpu_strat == "async"


@njit(cache=caching)
def gpu_loop_source(seed, data, mcdc):

    # Progress bar indicator
    N_prog = 0

    if mcdc["technique"]["domain_decomposition"]:
        kernel.dd_check_in(mcdc)

    # =====================================================================
    # GPU Interop
    # =====================================================================

    # For async execution
    iter_count = 655360000
    # For event-based execution
    batch_size = 64

    full_work_size = mcdc["mpi_work_size"]
    if ASYNC_EXECUTION:
        phase_size = 1000000000
    else:
        phase_size = 1000000
    phase_count = (full_work_size + phase_size - 1) // phase_size

    for phase in range(phase_count):

        mcdc["mpi_work_iter"][0] = phase_size * phase
        mcdc["mpi_work_size"] = min(phase_size * (phase + 1), full_work_size)
        mcdc["source_seed"] = seed

        # Store the global state to the GPU
        if config.gpu_state_storage == "separate":
            adapt.harm.memcpy_host_to_device(mcdc["gpu_meta"]["state_pointer"], mcdc)
            adapt.harm.memcpy_host_to_device(mcdc["gpu_meta"]["state_pointer"], data)

        # Execute the program, and continue to do so until it is done
        if ASYNC_EXECUTION:
            src_exec_program(
                mcdc["gpu_meta"]["source_program_pointer"], BLOCK_COUNT, iter_count
            )
            while not src_complete(mcdc["gpu_meta"]["source_program_pointer"]):
                kernel.dd_particle_send(mcdc)
                src_exec_program(
                    mcdc["gpu_meta"]["source_program_pointer"], BLOCK_COUNT, iter_count
                )
        else:
            src_exec_program(
                mcdc["gpu_meta"]["source_program_pointer"], BLOCK_COUNT, batch_size
            )
            while not src_complete(mcdc["gpu_meta"]["source_program_pointer"]):
                kernel.dd_particle_send(mcdc)
                src_exec_program(
                    mcdc["gpu_meta"]["source_program_pointer"], BLOCK_COUNT, batch_size
                )
        src_clear_flags(mcdc["gpu_meta"]["source_program_pointer"])
        # Recover the original program state

        if config.gpu_state_storage == "separate":
            adapt.harm.memcpy_device_to_host(mcdc, mcdc["gpu_meta"]["state_pointer"])
            adapt.harm.memcpy_device_to_host(data, mcdc["gpu_meta"]["state_pointer"])

        src_clear_flags(mcdc["gpu_meta"]["source_program_pointer"])

    mcdc["mpi_work_size"] = full_work_size

    kernel.set_bank_size(mcdc["bank_active"], 0)

    # =====================================================================
    # Closeout (Moved out of the typical particle loop)
    # =====================================================================

    source_closeout(mcdc, 1, 1, data)

    if mcdc["technique"]["domain_decomposition"]:
        source_dd_resolution(data, mcdc)


# =========================================================================
# Particle loop
# =========================================================================


@njit
def loop_particle(P_arr, data_tally, prog, data):
    P = P_arr[0]
    mcdc = adapt.mcdc_global(prog)

    while P["alive"]:
        step_particle(P_arr, data_tally, prog, data)


@njit
def step_particle(P_arr, data_tally, prog, data):
    P = P_arr[0]
    mcdc = adapt.mcdc_global(prog)

    # Determine and move to event
    kernel.move_to_event(P_arr, data_tally, mcdc, data)

    # Execute events
    if P["event"] == EVENT_LOST:
        return

    # Collision
    if P["event"] & EVENT_COLLISION:
        # Branchless collision?
        if mcdc["technique"]["branchless_collision"]:
            kernel.branchless_collision(P_arr, prog)

        # Analog collision
        else:
            # Get collision type
            physics.collision(P_arr, prog, data)

    # Surface and domain crossing
    if P["event"] & EVENT_SURFACE_CROSSING:
        geometry.surface_crossing(P_arr, data_tally, prog)
        if P["event"] & EVENT_DOMAIN_CROSSING:
            if mcdc["surfaces"][P["surface_ID"]]["BC"] == BC_NONE:
                kernel.domain_crossing(P_arr, prog)

    elif P["event"] & EVENT_DOMAIN_CROSSING:
        kernel.domain_crossing(P_arr, prog)

    # Census time crossing
    if P["event"] & EVENT_TIME_CENSUS:
        adapt.add_census(P_arr, prog)
        P["alive"] = False

    # Time boundary crossing
    if P["event"] & EVENT_TIME_BOUNDARY:
        P["alive"] = False

    # Apply weight window
    if P["alive"] and mcdc["technique"]["weight_window"]:
        kernel.weight_window(P_arr, prog)

    # Apply weight roulette
    if P["alive"] and mcdc["technique"]["weight_roulette"]:
        # check if weight has fallen below threshold
        if abs(P["w"]) <= mcdc["technique"]["wr_threshold"]:
            kernel.weight_roulette(P_arr, mcdc)


def build_gpu_progs(input_deck, args):

    STRAT = args.gpu_strat

    src_spec = gpu_sources_spec()

    adapt.harm.RuntimeSpec.bind_specs()

    rank = MPI.COMM_WORLD.Get_rank()
    device_id = rank % args.gpu_share_stride

    if MPI.COMM_WORLD.Get_size() > 1:
        MPI.COMM_WORLD.Barrier()

    adapt.harm.RuntimeSpec.load_specs()

    if STRAT == "async":
        args.gpu_arena_size = args.gpu_arena_size // 32
        src_fns = src_spec.async_functions()
        pre_fns = pre_spec.async_functions()
    else:
        src_fns = src_spec.event_functions()
        pre_fns = pre_spec.event_functions()

    ARENA_SIZE = args.gpu_arena_size
    BLOCK_COUNT = args.gpu_block_count

    global alloc_state, free_state
    alloc_state = src_fns["alloc_state"]
    free_state = src_fns["free_state"]

    global src_alloc_program, src_free_program
    global src_load_global, src_store_global, src_load_data, src_store_data, src_store_pointer_data
    global src_init_program, src_exec_program, src_complete, src_clear_flags
    src_alloc_program = src_fns["alloc_program"]
    src_free_program = src_fns["free_program"]
    src_load_global = src_fns["load_state_device_global"]
    src_store_global = src_fns["store_state_device_global"]
    src_store_pointer_global = src_fns["store_pointer_state_device_global"]
    src_load_data = src_fns["load_state_device_data"]
    src_store_data = src_fns["store_state_device_data"]
    src_store_pointer_data = src_fns["store_pointer_state_device_data"]
    src_init_program = src_fns["init_program"]
    src_exec_program = src_fns["exec_program"]
    src_complete = src_fns["complete"]
    src_clear_flags = src_fns["clear_flags"]
    src_set_device = src_fns["set_device"]

    global pre_alloc_program, pre_free_program
    global pre_load_global, pre_store_global, pre_load_data, pre_store_data
    global pre_init_program, pre_exec_program, pre_complete, pre_clear_flags
    pre_alloc_state = pre_fns["alloc_state"]
    pre_free_state = pre_fns["free_state"]
    pre_alloc_program = pre_fns["alloc_program"]
    pre_free_program = pre_fns["free_program"]
    pre_load_global = pre_fns["load_state_device_global"]
    pre_store_global = pre_fns["store_state_device_global"]
    pre_load_data = pre_fns["load_state_device_data"]
    pre_store_data = pre_fns["store_state_device_data"]
    pre_init_program = pre_fns["init_program"]
    pre_exec_program = pre_fns["exec_program"]
    pre_complete = pre_fns["complete"]
    pre_clear_flags = pre_fns["clear_flags"]

    @njit
    def real_setup_gpu(mcdc_array, data_tally):
        mcdc = mcdc_array[0]
        src_set_device(device_id)
        arena_size = ARENA_SIZE
        mcdc["gpu_meta"]["state_pointer"] = adapt.cast_voidptr_to_uintp(alloc_state())
        # src_store_global(mcdc["gpu_meta"]["state_pointer"], mcdc_array[0])
        if config.gpu_state_storage == "separate":
            src_store_pointer_global(
                mcdc["gpu_meta"]["state_pointer"], mcdc["gpu_meta"]["global_pointer"]
            )
            src_store_pointer_data(
                mcdc["gpu_meta"]["state_pointer"], mcdc["gpu_meta"]["tally_pointer"]
            )
        else:
            src_store_pointer_global(mcdc["gpu_meta"]["state_pointer"], mcdc_array)
            src_store_pointer_data(mcdc["gpu_meta"]["state_pointer"], data_tally)

        mcdc["gpu_meta"]["source_program_pointer"] = adapt.cast_voidptr_to_uintp(
            src_alloc_program(mcdc["gpu_meta"]["state_pointer"], ARENA_SIZE)
        )
        src_init_program(mcdc["gpu_meta"]["source_program_pointer"], BLOCK_COUNT)
        return

    @njit
    def real_teardown_gpu(mcdc):
        src_free_program(
            adapt.cast_uintp_to_voidptr(mcdc["gpu_meta"]["source_program_pointer"])
        )
        free_state(adapt.cast_uintp_to_voidptr(mcdc["gpu_meta"]["state_pointer"]))

    global setup_gpu, teardown_gpu
    setup_gpu = real_setup_gpu
    teardown_gpu = real_teardown_gpu

    global loop_source
    loop_source = gpu_loop_source
